{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe99bc0-b98f-4f98-b57a-c10e4beaa4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3275c07-a215-4966-9c5c-224743ac4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, groups=1,\n",
    "                 reduction=0.0625, kernel_num=4, min_channel=16):\n",
    "        super(Attention, self).__init__()\n",
    "        attention_channel = max(int(in_planes * reduction), min_channel)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel_num = kernel_num\n",
    "        self.temperature = 30.0\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Conv2d(in_planes, attention_channel, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(attention_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.channel_fc = nn.Conv2d(attention_channel, in_planes, 1, bias=True)\n",
    "        self.func_channel = self.get_channel_attention\n",
    "\n",
    "        if in_planes == groups and in_planes == out_planes:  # depth-wise convolution\n",
    "            self.func_filter = self.skip\n",
    "        else:\n",
    "            self.filter_fc = nn.Conv2d(attention_channel, out_planes, 1, bias=True)\n",
    "            self.func_filter = self.get_filter_attention\n",
    "\n",
    "        if kernel_size == 1:  # point-wise convolution\n",
    "            self.func_spatial = self.skip\n",
    "        else:\n",
    "            self.spatial_fc = nn.Conv2d(attention_channel, kernel_size * kernel_size, 1, bias=True)\n",
    "            self.func_spatial = self.get_spatial_attention\n",
    "\n",
    "        if kernel_num == 1:\n",
    "            self.func_kernel = self.skip\n",
    "        else:\n",
    "            self.kernel_fc = nn.Conv2d(attention_channel, kernel_num, 1, bias=True)\n",
    "            self.func_kernel = self.get_kernel_attention\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def update_temperature(self, temperature):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def skip(_):\n",
    "        return 1.0\n",
    "\n",
    "    def get_channel_attention(self, x):\n",
    "        channel_attention = torch.sigmoid(self.channel_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n",
    "        return channel_attention\n",
    "\n",
    "    def get_filter_attention(self, x):\n",
    "        filter_attention = torch.sigmoid(self.filter_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n",
    "        return filter_attention\n",
    "\n",
    "    def get_spatial_attention(self, x):\n",
    "        spatial_attention = self.spatial_fc(x).view(x.size(0), 1, 1, 1, self.kernel_size, self.kernel_size)\n",
    "        spatial_attention = torch.sigmoid(spatial_attention / self.temperature)\n",
    "        return spatial_attention\n",
    "\n",
    "    def get_kernel_attention(self, x):\n",
    "        kernel_attention = self.kernel_fc(x).view(x.size(0), -1, 1, 1, 1, 1)\n",
    "        kernel_attention = F.softmax(kernel_attention / self.temperature, dim=1)\n",
    "        return kernel_attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return self.func_channel(x), self.func_filter(x), self.func_spatial(x), self.func_kernel(x)\n",
    "\n",
    "\n",
    "class ODConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n",
    "                 reduction=0.0625, kernel_num=4):\n",
    "        super(ODConv2d, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.kernel_num = kernel_num\n",
    "        self.attention = Attention(in_planes, out_planes, kernel_size, groups=groups,\n",
    "                                   reduction=reduction, kernel_num=kernel_num)\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_num, out_planes, in_planes//groups, kernel_size, kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if self.kernel_size == 1 and self.kernel_num == 1:\n",
    "            self._forward_impl = self._forward_impl_pw1x\n",
    "        else:\n",
    "            self._forward_impl = self._forward_impl_common\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for i in range(self.kernel_num):\n",
    "            nn.init.kaiming_normal_(self.weight[i], mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def update_temperature(self, temperature):\n",
    "        self.attention.update_temperature(temperature)\n",
    "\n",
    "    def _forward_impl_common(self, x):\n",
    "        # Multiplying channel attention (or filter attention) to weights and feature maps are equivalent,\n",
    "        # while we observe that when using the latter method the models will run faster with less gpu memory cost.\n",
    "        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)\n",
    "        batch_size, in_planes, height, width = x.size()\n",
    "        x = x * channel_attention\n",
    "        x = x.reshape(1, -1, height, width)\n",
    "        aggregate_weight = spatial_attention * kernel_attention * self.weight.unsqueeze(dim=0)\n",
    "        aggregate_weight = torch.sum(aggregate_weight, dim=1).view(\n",
    "            [-1, self.in_planes // self.groups, self.kernel_size, self.kernel_size])\n",
    "        output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,\n",
    "                          dilation=self.dilation, groups=self.groups * batch_size)\n",
    "        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))\n",
    "        output = output * filter_attention\n",
    "        return output\n",
    "\n",
    "    def _forward_impl_pw1x(self, x):\n",
    "        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)\n",
    "        x = x * channel_attention\n",
    "        output = F.conv2d(x, weight=self.weight.squeeze(dim=0), bias=None, stride=self.stride, padding=self.padding,\n",
    "                          dilation=self.dilation, groups=self.groups)\n",
    "        output = output * filter_attention\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4328d5-26aa-4328-bafe-2c404b92b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1,\n",
    "                 groups=1, norm_layer=nn.BatchNorm2d,\n",
    "                 reduction=0.0625, kernel_num=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ODConvBN, self).__init__(\n",
    "            ODConv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups,\n",
    "                     reduction=reduction, kernel_num=kernel_num),\n",
    "            norm_layer(out_planes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511f3d35-94a9-4e55-ab6d-318a4de53397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV3 Model as defined in:\n",
    "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\n",
    "Searching for MobileNetV3\n",
    "arXiv preprint arXiv:1905.02244.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernel_size = 3, stride = stride,\n",
    "                            padding = 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernel_size = 1, stride = 1,\n",
    "                            padding = 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                ODConvBN(hidden_dim, hidden_dim, kernel_size, stride, groups=hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                ODConv2d(hidden_dim, oup, 1, 1),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                ODConvBN(inp, hidden_dim, kernel_size = 1, stride = 1),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                ODConvBN(hidden_dim, hidden_dim, kernel_size, stride, groups=hidden_dim),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                ODConv2d(hidden_dim, oup, 1, 1),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, mode, num_classes=1000, width_mult=1.):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        assert mode in ['large', 'small']\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(16 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = InvertedResidual\n",
    "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            exp_size = _make_divisible(input_channel * t, 8)\n",
    "            layers.append(block(input_channel, exp_size, output_channel, k, s, use_se, use_hs))\n",
    "            input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # building last several layers\n",
    "        self.conv = conv_1x1_bn(input_channel, exp_size)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_channel = {'large': 1280, 'small': 1024}\n",
    "\n",
    "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(exp_size, output_channel),\n",
    "            h_swish(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(output_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def mobilenetv3_large(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Large model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t,   c,  SE, HS, s\n",
    "        [3,   1,  16, 0, 0, 1],\n",
    "        [3,   4,  24, 0, 0, 2],\n",
    "        [3,   3,  24, 0, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 2],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [3,   6,  80, 0, 1, 2],\n",
    "        [3, 2.5,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 2],\n",
    "        [5,   6, 160, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 1]\n",
    "    ]\n",
    "    return MobileNetV3(cfgs, mode='large', **kwargs)\n",
    "\n",
    "\n",
    "def mobilenetv3_small(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Small model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k,   t,  c, SE, HS, s\n",
    "        [3,    1,  16, 1, 0, 2],\n",
    "        [3,  4.5,  24, 0, 0, 2],\n",
    "        [3, 3.67,  24, 0, 0, 1],\n",
    "        [5,    4,  40, 1, 1, 2],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 2],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "    ]\n",
    "\n",
    "    return MobileNetV3(cfgs, mode='small', **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88ade2bc-b3d5-4ad3-bf8e-96f3472d5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def69fa4-e356-43d1-af2e-e5889993a42c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2ae798-ceb6-4610-ba64-304aba3e1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7269b1e-25b6-4c82-a33f-b1f83dadff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170498071/170498071 [00:23<00:00, 7226753.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir, download = True):\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])\n",
    "\n",
    "  train_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = True,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  test_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = False,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  return (train_data, test_data)\n",
    "\n",
    "train_data, test_data = load_data('./data/cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6025fb9-496d-4820-ba81-dfe7cd7975f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size,\n",
    "                          shuffle = True, num_workers = num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size,\n",
    "                         shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea2c6b43-6a2e-4230-8b6e-8575fd80788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6046ad77-f972-4ce6-970b-16c5ababca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, device, alpha = 1.0):\n",
    "  if alpha > 0:\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "  else:\n",
    "    lam = 1\n",
    "\n",
    "  batch_size = x.shape[0]\n",
    "  index_sample = torch.randperm(batch_size).to(device)\n",
    "\n",
    "  mixed_x = lam * x + (1 - lam) * x[index_sample, :]\n",
    "  y, y_sampled =  y, y[index_sample]\n",
    "\n",
    "  return mixed_x, y, y_sampled, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "  return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0982adb8-9ddd-47aa-8c2c-238bdcf0544e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new directory\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def check_logging_directory(path):\n",
    "  parent_directory = os.path.dirname(path)\n",
    "  if not os.path.exists(parent_directory):\n",
    "    os.makedirs(parent_directory)\n",
    "    print(\"Create new directory\")\n",
    "\n",
    "logging_path = './logging/mixup_mobilenetv3_omni.log'\n",
    "check_logging_directory(logging_path)\n",
    "\n",
    "logging.basicConfig(filename=logging_path, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa95a1d0-fe3c-4078-85d5-06439b7f793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2396571"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "mb_v3 =  mobilenetv3_small(num_classes = 10).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(mb_v3.parameters(), lr=learning_rate,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "count_parameters(mb_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded28beb-1775-4dd7-96fb-af835863e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "               position = 1, leave = True)\n",
    "\n",
    "print(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "logging.info(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    mb_v3.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        X, y_origin, y_sampled, lam = mixup_data(X, y, device, alpha = 0.4)\n",
    "\n",
    "        # Forward pass\n",
    "        output = mb_v3(X)\n",
    "        # loss = criterion(output, y)\n",
    "        loss = mixup_criterion(criterion, output, y_origin, y_sampled, lam)\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating the accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        n_correct = (lam * predicted.eq(y_origin.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted.eq(y_sampled.data).cpu().sum().float())\n",
    "\n",
    "        acc_t = n_correct / len(predicted) * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        total_acc += n_correct\n",
    "        total += y.shape[0]\n",
    "\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\t|\\tTrain Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    logging.info(\"========================================\")\n",
    "    logging.info(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    logging.info(f\"Train Loss: {current_loss:.2f}\\nTrain Acc: {current_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    # Eval trÃªn valid set\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    mb_v3.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            output = mb_v3(X)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(output, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "            total_loss += loss_t\n",
    "\n",
    "            # Calculate Accuracies\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_correct = (predicted == y).sum().item()\n",
    "            acc_t = n_correct / len(predicted) * 100\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "            total_acc += n_correct\n",
    "\n",
    "            total += y.shape[0]\n",
    "\n",
    "            val_bar.set_postfix(loss = running_loss,\n",
    "                                acc = f\"{running_acc:.2f}%\",\n",
    "                                epoch = epoch + 1)\n",
    "            val_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(test_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "\n",
    "    print(f\"Val Loss: {current_loss:.2f}\\t|\\tVal Acc: {current_acc:.2f}%\")\n",
    "    logging.info(f\"Val Loss: {current_loss:.2f}\\nVal Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"Training Completed! ðŸ˜€\")\n",
    "logging.info(\"========================================\")\n",
    "logging.info(\"Training Completed! ðŸ˜€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
