{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6c71ed-943e-44a4-be1b-a2e2991f3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, groups=1,\n",
    "                 reduction = 0.0625, kernel_num=4, min_channel=16,\n",
    "                 temperature = 60.0):\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "        attention_channel = max(int(in_planes * reduction), min_channel)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel_num = kernel_num\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Conv2d(in_planes, attention_channel, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(attention_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.channel_fc = nn.Conv2d(attention_channel, in_planes, 1, bias=True)\n",
    "        self.func_channel = self.get_channel_attention\n",
    "\n",
    "        if in_planes == groups and in_planes == out_planes:  # depth-wise convolution\n",
    "            self.func_filter = self.skip\n",
    "        else:\n",
    "            self.filter_fc = nn.Conv2d(attention_channel, out_planes,\n",
    "                                       1, bias=True)\n",
    "            self.func_filter = self.get_filter_attention\n",
    "\n",
    "        if kernel_size == 1:  # point-wise convolution\n",
    "            self.func_spatial = self.skip\n",
    "        else:\n",
    "            self.spatial_fc = nn.Conv2d(attention_channel, kernel_size * kernel_size,\n",
    "                                        1, bias=True)\n",
    "            self.func_spatial = self.get_spatial_attention\n",
    "\n",
    "        if kernel_num == 1:\n",
    "            self.func_kernel = self.skip\n",
    "        else:\n",
    "            self.kernel_fc = nn.Conv2d(attention_channel, kernel_num,\n",
    "                                       1, bias=True)\n",
    "            self.func_kernel = self.get_kernel_attention\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def update_temperature(self, temperature):\n",
    "        self.temperature = temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def skip(_):\n",
    "        return 1.0\n",
    "\n",
    "    def get_channel_attention(self, x):\n",
    "        channel_attention = torch.sigmoid(self.channel_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n",
    "        return channel_attention\n",
    "\n",
    "    def get_filter_attention(self, x):\n",
    "        filter_attention = torch.sigmoid(self.filter_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n",
    "        return filter_attention\n",
    "\n",
    "    def get_spatial_attention(self, x):\n",
    "        spatial_attention = self.spatial_fc(x).view(x.size(0), 1, 1, 1, self.kernel_size, self.kernel_size)\n",
    "        spatial_attention = torch.sigmoid(spatial_attention / self.temperature)\n",
    "        return spatial_attention\n",
    "\n",
    "    def get_kernel_attention(self, x):\n",
    "        kernel_attention = self.kernel_fc(x).view(x.size(0), -1, 1, 1, 1, 1)\n",
    "        kernel_attention = F.softmax(kernel_attention / self.temperature, dim=1)\n",
    "        return kernel_attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return self.func_channel(x), self.func_filter(x), self.func_spatial(x), self.func_kernel(x)\n",
    "\n",
    "\n",
    "class ODConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1,\n",
    "                 reduction = 0.0625, kernel_num=4,\n",
    "                 temperature = 60.0):\n",
    "        super(ODConv2d, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.kernel_num = kernel_num\n",
    "        self.attention = Attention(in_planes, out_planes,\n",
    "                                   kernel_size, groups=groups,\n",
    "                                   reduction=reduction,\n",
    "                                   kernel_num=kernel_num,\n",
    "                                   temperature = temperature)\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_num, out_planes,\n",
    "                                               in_planes//groups, kernel_size, kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if self.kernel_size == 1 and self.kernel_num == 1:\n",
    "            self._forward_impl = self._forward_impl_pw1x\n",
    "        else:\n",
    "            self._forward_impl = self._forward_impl_common\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for i in range(self.kernel_num):\n",
    "            nn.init.kaiming_normal_(self.weight[i], mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def update_temperature(self, temperature):\n",
    "        self.attention.update_temperature(temperature)\n",
    "\n",
    "    def get_temperature(self):\n",
    "        return self.attention.temperature\n",
    "\n",
    "    def _forward_impl_common(self, x):\n",
    "        # Multiplying channel attention (or filter attention) to weights and feature maps are equivalent,\n",
    "        # while we observe that when using the latter method the models will run faster with less gpu memory cost.\n",
    "        channel_attention, filter_attention,  \\\n",
    "            spatial_attention, kernel_attention = self.attention(x)\n",
    "\n",
    "        batch_size, in_planes, height, width = x.size()\n",
    "        x = x * channel_attention\n",
    "        x = x.reshape(1, -1, height, width)\n",
    "        aggregate_weight = spatial_attention * kernel_attention * \\\n",
    "                         self.weight.unsqueeze(dim=0)\n",
    "        aggregate_weight = torch.sum(aggregate_weight, dim=1).view(\n",
    "            [-1, self.in_planes // self.groups, self.kernel_size, self.kernel_size])\n",
    "        output = F.conv2d(x, weight = aggregate_weight, bias = None,\n",
    "                          stride = self.stride, padding = self.padding,\n",
    "                          dilation = self.dilation,\n",
    "                          groups = self.groups * batch_size)\n",
    "        output = output.view(batch_size, self.out_planes,\n",
    "                             output.size(-2), output.size(-1))\n",
    "        output = output * filter_attention\n",
    "        return output\n",
    "\n",
    "    def _forward_impl_pw1x(self, x):\n",
    "        channel_attention, filter_attention, \\\n",
    "            spatial_attention, kernel_attention = self.attention(x)\n",
    "        x = x * channel_attention\n",
    "        output = F.conv2d(x, weight=self.weight.squeeze(dim=0),\n",
    "                          bias=None, stride=self.stride, padding=self.padding,\n",
    "                          dilation=self.dilation, groups=self.groups)\n",
    "        output = output * filter_attention\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "class ODConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1,\n",
    "                 groups=1, norm_layer=nn.BatchNorm2d,\n",
    "                 reduction=0.0625, kernel_num = 1, temperature = 60.0):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ODConvBN, self).__init__(\n",
    "            ODConv2d(in_planes, out_planes, kernel_size,\n",
    "                     stride, padding, groups = groups,\n",
    "                     reduction = reduction,\n",
    "                     kernel_num = kernel_num, temperature = temperature),\n",
    "            norm_layer(out_planes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1263b6-e160-483d-b29e-9f7ed04ede37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a MobileNetV3 Model as defined in:\n",
    "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam. (2019).\n",
    "Searching for MobileNetV3\n",
    "arXiv preprint arXiv:1905.02244.\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "__all__ = ['mobilenetv3_large', 'mobilenetv3_small']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(channel, _make_divisible(channel // reduction, 8)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(_make_divisible(channel // reduction, 8), channel),\n",
    "                h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d8c300-2e88-4c8f-9895-b5cf91b51093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_3x3(inp, oup, stride, batch_norm = True):\n",
    "    layers = [\n",
    "        nn.Conv2d(inp, oup, kernel_size=3, stride=1,\n",
    "                  padding=1, bias=False),\n",
    "        h_swish()\n",
    "    ]#\n",
    "    if batch_norm:\n",
    "        layers.insert(1, nn.BatchNorm2d(oup))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv_1x1(inp, oup, batch_norm = True):\n",
    "    layers = [\n",
    "        nn.Conv2d(inp, oup, kernel_size=1, stride=1,\n",
    "                  padding=0, bias=False),\n",
    "        h_swish()\n",
    "    ]\n",
    "    if batch_norm:\n",
    "        layers.insert(1, nn.BatchNorm2d(oup))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb6bdb34-c7e8-44ca-873c-653a580a3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_conv_1x1(inp, oup, stride = 1, kernel_num = 4,\n",
    "                temperature = 60,\n",
    "                batch_norm = True):\n",
    "    return nn.Sequential(\n",
    "        ODConvBN(inp, oup, kernel_size = 1, stride = stride,\n",
    "            kernel_num = kernel_num, temperature = temperature) \\\n",
    "                if batch_norm == True else \\\n",
    "        ODConv2d(inp, oup, kernel_size = 1, stride = stride,\n",
    "            kernel_num = kernel_num, temperature = temperature),\n",
    "\n",
    "        h_swish()\n",
    "    )\n",
    "\n",
    "def od_conv_3x3(inp, oup, stride = 1,\n",
    "                kernel_num = 4, temperature = 60,\n",
    "                batch_norm = True):\n",
    "    return nn.Sequential(\n",
    "         ODConvBN(inp, oup, kernel_size = 3, stride = stride,\n",
    "            kernel_num = kernel_num, temperature = temperature) \\\n",
    "                if batch_norm == True else \\\n",
    "        ODConv2d(inp, oup, kernel_size = 3, stride = stride,\n",
    "            kernel_num = kernel_num, temperature = temperature),\n",
    "\n",
    "        h_swish()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e3ebd2-b581-4112-b65a-4f4beac8a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualOD(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size,\n",
    "                 stride, use_se, use_hs,\n",
    "                 kernel_num = 4, temperature = 60.0):\n",
    "        super(InvertedResidualOD, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        print(\"Using OmniDimensional\")\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                ODConvBN(hidden_dim, hidden_dim, kernel_size, stride,\n",
    "                         groups=hidden_dim, kernel_num = kernel_num,\n",
    "                         temperature = temperature),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                ODConv2d(hidden_dim, oup, 1, 1, kernel_num = kernel_num,\n",
    "                         temperature = temperature),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                ODConvBN(inp, hidden_dim, kernel_size = 1, stride = 1,\n",
    "                         kernel_num = kernel_num, temperature = temperature),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                ODConvBN(hidden_dim, hidden_dim, kernel_size,\n",
    "                         stride, groups=hidden_dim, kernel_num = kernel_num,\n",
    "                         temperature = temperature),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                ODConv2d(hidden_dim, oup, 1, 1, kernel_num = kernel_num,\n",
    "                         temperature = temperature),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        print(\"Using Normal\")\n",
    "        if inp == hidden_dim:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, (kernel_size - 1) // 2, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                # Squeeze-and-Excite\n",
    "                SELayer(hidden_dim) if use_se else nn.Identity(),\n",
    "                h_swish() if use_hs else nn.ReLU(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb43612-ac74-4181-b76b-7b48cf287529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, cfgs, mode, kernel_num,\n",
    "                 temperature, od_bottleneck = 0, \n",
    "                 od_outside = 0, num_classes = 10, width_mult=1.,\n",
    "                 use_od = False, drop_rate = 0.2):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = cfgs\n",
    "        self.use_od = use_od\n",
    "        assert mode in ['large', 'small']\n",
    "\n",
    "        num_od = int(od_outside)\n",
    "        od_bottleneck = int(od_bottleneck)\n",
    "        \n",
    "        self.num_od = num_od\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(16 * width_mult, 8)\n",
    "\n",
    "        layers = []\n",
    "        if num_od > 0:\n",
    "            print(\"Using OD\")\n",
    "            layers.append(od_conv_3x3(3, input_channel, stride = 2,\n",
    "                                      kernel_num = kernel_num,\n",
    "                                      temperature = temperature))\n",
    "        else:\n",
    "            print(\"Using Normal\")\n",
    "            layers.append(conv_3x3(3, input_channel, stride = 2))\n",
    "        \n",
    "        # building inverted residual blocks\n",
    "        \n",
    "\n",
    "        i = 0\n",
    "        for k, t, c, use_se, use_hs, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            exp_size = _make_divisible(input_channel * t, 8)\n",
    "            block = InvertedResidual if (use_od == False or i >= od_bottleneck) \\\n",
    "                        else InvertedResidualOD\n",
    "            \n",
    "            if use_od == False or i >= od_bottleneck:\n",
    "                layers.append(block(input_channel, exp_size, output_channel,\n",
    "                                    k, s, use_se, use_hs))\n",
    "            else:\n",
    "                layers.append(block(input_channel, exp_size, output_channel,\n",
    "                                    k, s, use_se, use_hs, kernel_num = kernel_num,\n",
    "                                    temperature = temperature))\n",
    "                i += 1\n",
    "            \n",
    "            input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # building last several layers\n",
    "        if num_od >= 2:\n",
    "            print(\"Using OD\")\n",
    "            self.conv = od_conv_1x1(input_channel, exp_size,\n",
    "                                    kernel_num = kernel_num,\n",
    "                                    temperature = temperature)\n",
    "        else:\n",
    "            print(\"Using Normal\")\n",
    "            self.conv = conv_1x1(input_channel, exp_size)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        output_channel = {'large': 1280, 'small': 1024}\n",
    "\n",
    "        output_channel = _make_divisible(output_channel[mode] * width_mult, 8) if width_mult > 1.0 else output_channel[mode]\n",
    "\n",
    "        omni_layers = []\n",
    "        not_omni_layers = []\n",
    "        temp_od = num_od - 2\n",
    "\n",
    "        self.list_layers = []\n",
    "        for i in range(2):\n",
    "            if temp_od > 0:\n",
    "                self.list_layers.append(\"OD\")\n",
    "            else:\n",
    "                self.list_layers.append(\"Normal\")\n",
    "\n",
    "            temp_od -= 1\n",
    "\n",
    "\n",
    "        input_channel = exp_size\n",
    "        for layer in self.list_layers:\n",
    "            if layer == \"OD\":\n",
    "                print(\"Using OD\")\n",
    "                omni_layers.append(od_conv_1x1(input_channel, output_channel,\n",
    "                                          kernel_num = kernel_num,\n",
    "                                          temperature = temperature))\n",
    "            else:\n",
    "                print(\"Using Normal\")\n",
    "                not_omni_layers.append(nn.Linear(input_channel, output_channel))\n",
    "\n",
    "            input_channel = output_channel\n",
    "            output_channel = num_classes\n",
    "\n",
    "        self.omni_layers = nn.Sequential(*omni_layers)\n",
    "        self.normal_layers = nn.Sequential(*not_omni_layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.omni_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.normal_layers(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def net_update_temperature(self, temperature):\n",
    "        for modules in self.modules():\n",
    "            if hasattr(modules, \"update_temperature\"):\n",
    "                modules.update_temperature(temperature)\n",
    "\n",
    "    def display_temperature(self):\n",
    "        for modules in self.modules():\n",
    "            if hasattr(modules, \"get_temperature\"):\n",
    "                return modules.get_temperature()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378e115b-cf53-4063-8867-77ef6097d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenetv3_large(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Large model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k, t,   c,  SE, HS, s\n",
    "        [3,   1,  16, 0, 0, 1],\n",
    "        [3,   4,  24, 0, 0, 2],\n",
    "        [3,   3,  24, 0, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 2],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [5,   3,  40, 1, 0, 1],\n",
    "        [3,   6,  80, 0, 1, 2],\n",
    "        [3, 2.5,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3, 2.3,  80, 0, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [3,   6, 112, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 2],\n",
    "        [5,   6, 160, 1, 1, 1],\n",
    "        [5,   6, 160, 1, 1, 1]\n",
    "    ]\n",
    "    return MobileNetV3(cfgs, mode='large', **kwargs)\n",
    "\n",
    "\n",
    "def mobilenetv3_small(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a MobileNetV3-Small model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # k,   t,  c, SE, HS, s\n",
    "        [3,    1,  16, 1, 0, 2],\n",
    "        [3,  4.5,  24, 0, 0, 2],\n",
    "        [3, 3.67,  24, 0, 0, 1],\n",
    "        [5,    4,  40, 1, 1, 2],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    6,  40, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    3,  48, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 2],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "        [5,    6,  96, 1, 1, 1],\n",
    "    ]\n",
    "\n",
    "    return MobileNetV3(cfgs, mode='small', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e7d5fc-6d03-4cbc-bdfc-be74e7f6cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be475e5-ddff-423d-932d-533684d38954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b1f1bd7-c65f-455b-8264-144a362a9f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_dir, download = True):\n",
    "\n",
    "  transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "  ])\n",
    "\n",
    "  train_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = True,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  test_data = datasets.CIFAR10(\n",
    "      root = data_dir, train = False,\n",
    "      download = download, transform = transform\n",
    "  )\n",
    "\n",
    "  return (train_data, test_data)\n",
    "\n",
    "train_data, test_data = load_data('./data/cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d3807d-a31a-4eba-a8bb-5f1953e0398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "args = Namespace(\n",
    "    data_dir = './data/cifar10',\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    batch_size = 128,\n",
    "    num_workers = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08f62755-52a7-4e0f-8737-cfa8d7051e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = args.batch_size,\n",
    "                          shuffle = True, num_workers = args.num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size = args.batch_size,\n",
    "                         shuffle = True, num_workers = args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64061edf-8f9b-4ee6-9830-960bfd690ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def check_logging_directory(path):\n",
    "  parent_directory = os.path.dirname(path)\n",
    "  if not os.path.exists(parent_directory):\n",
    "    os.makedirs(parent_directory)\n",
    "    print(\"Create new directory\")\n",
    "\n",
    "logging_path = './logging/analysis.log'\n",
    "check_logging_directory(logging_path)\n",
    "\n",
    "logging.basicConfig(filename=logging_path, level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f88cd40-e9c7-48bc-a66c-d14e75045680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Normal\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "The number of parameters: 1805228\n"
     ]
    }
   ],
   "source": [
    "od_bottleneck = 6\n",
    "kernel_num = 4\n",
    "temperature = 30\n",
    "num_epochs = 30\n",
    "learning_rate = 0.05\n",
    "weight_decay = 0.00004\n",
    "dropout = 0.05\n",
    "momentum = 0.9\n",
    "\n",
    "mb_v3 = mobilenetv3_small(num_classes = 10, \n",
    "                          od_bottleneck = od_bottleneck,\n",
    "                          od_outside = 0,\n",
    "                          kernel_num = kernel_num, \n",
    "                          temperature = temperature,\n",
    "                          drop_rate = dropout,\n",
    "                          use_od = True).to(args.device)\n",
    "criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "optimizer = torch.optim.SGD(mb_v3.parameters(), lr=learning_rate,\n",
    "                            weight_decay = weight_decay, momentum = momentum)\n",
    "\n",
    "print(f\"The number of parameters: {count_parameters(mb_v3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7401ff51-c0c2-446b-a503-b4de86bcdddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, total_epochs,\n",
    "                         iteration, iter_per_epoch, initial_lr = 0.05):\n",
    "    current_iter = iteration + epoch * iter_per_epoch\n",
    "    max_iter = total_epochs * iter_per_epoch\n",
    "\n",
    "    lr = initial_lr * (1 + np.cos(np.pi * current_iter / max_iter)) / 2\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def get_temperature(iteration, epoch, iter_per_epoch,\n",
    "                        temp_epoch = 10, temp_init = 30.0):\n",
    "    total_temp_iter = iter_per_epoch * temp_epoch\n",
    "    current_iter = iteration + epoch * iter_per_epoch\n",
    "    # print(current_iter)\n",
    "    temperature = 1.0 + max(0, (temp_init - 1.0) * \\\n",
    "                            ((total_temp_iter - current_iter) / \\\n",
    "                            total_temp_iter))\n",
    "    return temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2349e0fe-503e-4c8f-aa06-6f670ab94adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a271a57240c4c2894f8dc1383615bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc467a1c0d334d2a9798ae8a0bf33efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b26736658945e5b7439a90b0999bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\n",
      "========================================\n",
      "\u001b[1;34mEpoch 1/30\u001b[0m\n",
      "Train Loss: 1.51\t|\tTrain Acc: 43.28%\n",
      "Val Loss: 1.28\t|\tVal Acc: 53.54%\n",
      "The current temperature is: 27.1\n",
      "The current learning rate is: 0.04998766400914329\n",
      "========================================\n",
      "\u001b[1;34mEpoch 2/30\u001b[0m\n",
      "Train Loss: 0.97\t|\tTrain Acc: 65.48%\n",
      "Val Loss: 1.07\t|\tVal Acc: 63.77%\n",
      "The current temperature is: 21.880000000000003\n",
      "The current learning rate is: 0.04995066821070679\n",
      "========================================\n",
      "\u001b[1;34mEpoch 3/30\u001b[0m\n",
      "Train Loss: 0.74\t|\tTrain Acc: 74.19%\n",
      "Val Loss: 0.78\t|\tVal Acc: 73.65%\n",
      "The current temperature is: 15.616000000000001\n",
      "The current learning rate is: 0.049889049115077\n",
      "========================================\n",
      "\u001b[1;34mEpoch 4/30\u001b[0m\n",
      "Train Loss: 0.60\t|\tTrain Acc: 79.36%\n",
      "Val Loss: 0.62\t|\tVal Acc: 78.37%\n",
      "The current temperature is: 9.7696\n",
      "The current learning rate is: 0.04980286753286195\n",
      "========================================\n",
      "\u001b[1;34mEpoch 5/30\u001b[0m\n",
      "Train Loss: 0.52\t|\tTrain Acc: 82.06%\n",
      "Val Loss: 0.56\t|\tVal Acc: 81.23%\n",
      "The current temperature is: 5.3848\n",
      "The current learning rate is: 0.04969220851487845\n",
      "========================================\n",
      "\u001b[1;34mEpoch 6/30\u001b[0m\n",
      "Train Loss: 0.87\t|\tTrain Acc: 70.22%\n",
      "Val Loss: 1.03\t|\tVal Acc: 64.73%\n",
      "The current temperature is: 2.75392\n",
      "The current learning rate is: 0.049557181268217225\n",
      "========================================\n",
      "\u001b[1;34mEpoch 7/30\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 76.00%\n",
      "Val Loss: 0.65\t|\tVal Acc: 78.17%\n",
      "The current temperature is: 1.526176\n",
      "The current learning rate is: 0.049397919048468686\n",
      "========================================\n",
      "\u001b[1;34mEpoch 8/30\u001b[0m\n",
      "Train Loss: 0.57\t|\tTrain Acc: 80.40%\n",
      "Val Loss: 0.96\t|\tVal Acc: 76.08%\n",
      "The current temperature is: 1.1052352\n",
      "The current learning rate is: 0.04921457902821578\n",
      "========================================\n",
      "\u001b[1;34mEpoch 9/30\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 76.07%\n",
      "Val Loss: 1.41\t|\tVal Acc: 50.93%\n",
      "The current temperature is: 1.01052352\n",
      "The current learning rate is: 0.04900734214192358\n",
      "========================================\n",
      "\u001b[1;34mEpoch 10/30\u001b[0m\n",
      "Train Loss: 0.82\t|\tTrain Acc: 71.69%\n",
      "Val Loss: 0.70\t|\tVal Acc: 76.33%\n",
      "The current temperature is: 1.0\n",
      "The current learning rate is: 0.048776412907378844\n",
      "========================================\n",
      "\u001b[1;34mEpoch 11/30\u001b[0m\n",
      "Train Loss: 0.68\t|\tTrain Acc: 76.58%\n",
      "Val Loss: 0.62\t|\tVal Acc: 78.73%\n",
      "The current temperature is: 1.0\n",
      "The current learning rate is: 0.04852201922385564\n",
      "========================================\n",
      "\u001b[1;34mEpoch 12/30\u001b[0m\n",
      "Train Loss: 0.59\t|\tTrain Acc: 79.56%\n",
      "Val Loss: 0.72\t|\tVal Acc: 75.27%\n",
      "The current temperature is: 1.0\n",
      "The current learning rate is: 0.04824441214720629\n",
      "========================================\n",
      "\u001b[1;34mEpoch 13/30\u001b[0m\n",
      "Train Loss: 0.49\t|\tTrain Acc: 83.09%\n",
      "Val Loss: 0.64\t|\tVal Acc: 78.73%\n",
      "The current temperature is: 1.0\n",
      "The current learning rate is: 0.04794386564209953\n",
      "========================================\n",
      "\u001b[1;34mEpoch 14/30\u001b[0m\n",
      "Train Loss: 0.41\t|\tTrain Acc: 85.97%\n",
      "Val Loss: 0.55\t|\tVal Acc: 81.56%\n",
      "The current temperature is: 1.0\n",
      "The current learning rate is: 0.04762067631165049\n",
      "========================================\n",
      "\u001b[1;34mEpoch 15/30\u001b[0m\n",
      "Train Loss: 0.36\t|\tTrain Acc: 87.53%\n",
      "Val Loss: 0.79\t|\tVal Acc: 76.52%\n",
      "The current temperature is: 1.0\n",
      "The current learning rate is: 0.047275163104709195\n",
      "========================================\n",
      "\u001b[1;34mEpoch 16/30\u001b[0m\n",
      "Train Loss: 0.32\t|\tTrain Acc: 88.96%\n",
      "Val Loss: 0.83\t|\tVal Acc: 79.26%\n",
      "The current learning rate is: 0.04690766700109659\n",
      "========================================\n",
      "\u001b[1;34mEpoch 17/30\u001b[0m\n",
      "Train Loss: 0.28\t|\tTrain Acc: 90.08%\n",
      "Val Loss: 0.53\t|\tVal Acc: 82.78%\n",
      "The current learning rate is: 0.046518550675098594\n",
      "========================================\n",
      "\u001b[1;34mEpoch 18/30\u001b[0m\n",
      "Train Loss: 0.24\t|\tTrain Acc: 91.38%\n",
      "Val Loss: 0.56\t|\tVal Acc: 83.13%\n",
      "The current learning rate is: 0.04610819813755038\n",
      "========================================\n",
      "\u001b[1;34mEpoch 19/30\u001b[0m\n",
      "Train Loss: 0.22\t|\tTrain Acc: 92.12%\n",
      "Val Loss: 0.66\t|\tVal Acc: 80.46%\n",
      "The current learning rate is: 0.04567701435686405\n",
      "========================================\n",
      "\u001b[1;34mEpoch 20/30\u001b[0m\n",
      "Train Loss: 0.20\t|\tTrain Acc: 93.11%\n",
      "Val Loss: 0.55\t|\tVal Acc: 83.43%\n",
      "The current learning rate is: 0.04522542485937369\n",
      "========================================\n",
      "\u001b[1;34mEpoch 21/30\u001b[0m\n",
      "Train Loss: 0.17\t|\tTrain Acc: 93.86%\n",
      "Val Loss: 0.60\t|\tVal Acc: 83.22%\n",
      "The current learning rate is: 0.04475387530939226\n",
      "========================================\n",
      "\u001b[1;34mEpoch 22/30\u001b[0m\n",
      "Train Loss: 0.16\t|\tTrain Acc: 94.30%\n",
      "Val Loss: 0.70\t|\tVal Acc: 81.05%\n",
      "The current learning rate is: 0.044262831069394735\n",
      "========================================\n",
      "\u001b[1;34mEpoch 23/30\u001b[0m\n",
      "Train Loss: 0.14\t|\tTrain Acc: 95.00%\n",
      "Val Loss: 0.61\t|\tVal Acc: 83.13%\n",
      "The current learning rate is: 0.043752776740761494\n",
      "========================================\n",
      "\u001b[1;34mEpoch 24/30\u001b[0m\n",
      "Train Loss: 0.12\t|\tTrain Acc: 95.69%\n",
      "Val Loss: 0.67\t|\tVal Acc: 83.08%\n",
      "The current learning rate is: 0.04322421568553529\n",
      "========================================\n",
      "\u001b[1;34mEpoch 25/30\u001b[0m\n",
      "Train Loss: 0.11\t|\tTrain Acc: 96.08%\n",
      "Val Loss: 0.61\t|\tVal Acc: 84.26%\n",
      "The current learning rate is: 0.04267766952966369\n",
      "========================================\n",
      "\u001b[1;34mEpoch 26/30\u001b[0m\n",
      "Train Loss: 0.10\t|\tTrain Acc: 96.49%\n",
      "Val Loss: 0.70\t|\tVal Acc: 82.45%\n",
      "The current learning rate is: 0.04211367764821722\n",
      "========================================\n",
      "\u001b[1;34mEpoch 27/30\u001b[0m\n",
      "Train Loss: 0.09\t|\tTrain Acc: 96.92%\n",
      "Val Loss: 0.64\t|\tVal Acc: 84.01%\n",
      "The current learning rate is: 0.0415327966330913\n",
      "========================================\n",
      "\u001b[1;34mEpoch 28/30\u001b[0m\n",
      "Train Loss: 0.08\t|\tTrain Acc: 97.37%\n",
      "Val Loss: 0.71\t|\tVal Acc: 83.61%\n",
      "The current learning rate is: 0.040935599743717244\n",
      "========================================\n",
      "\u001b[1;34mEpoch 29/30\u001b[0m\n",
      "Train Loss: 0.08\t|\tTrain Acc: 97.30%\n",
      "Val Loss: 0.75\t|\tVal Acc: 82.54%\n",
      "The current learning rate is: 0.040322676341324415\n",
      "========================================\n",
      "\u001b[1;34mEpoch 30/30\u001b[0m\n",
      "Train Loss: 0.08\t|\tTrain Acc: 97.13%\n",
      "Val Loss: 0.72\t|\tVal Acc: 83.57%\n",
      "The current learning rate is: 0.03969463130731183\n",
      "========================================\n",
      "Training Completed! ðŸ˜€\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "               position = 1, leave = True)\n",
    "print(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    mb_v3.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "        if epoch < 10:\n",
    "            temp = get_temperature(i + 1, epoch, len(train_loader),\n",
    "                                   temp_epoch = 10, temp_init = temperature)\n",
    "            mb_v3.net_update_temperature(temp)\n",
    "            # print(f\"The temperature is: {mb_v3.display_temperature()}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        X, y = X.to(args.device), y.to(args.device)\n",
    "\n",
    "        # X, y_origin, y_sampled, lam = mixup_data(X, y, args.device,\n",
    "        #                                          alpha = 0.4)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = mb_v3(X)\n",
    "        loss = criterion(output, y)\n",
    "        # loss = mixup_criterion(criterion, output, y_origin, y_sampled, lam)\n",
    "\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        temp_lr = adjust_learning_rate(optimizer, epoch, 100,\n",
    "                                       i + 1, len(train_loader),\n",
    "                                       initial_lr = learning_rate)\n",
    "\n",
    "        # Calculating the accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # n_correct = (lam * predicted.eq(y_origin.data).cpu().sum().float()\n",
    "        #             + (1 - lam) * predicted.eq(y_sampled.data).cpu().sum().float())\n",
    "        n_correct = (predicted == y).sum().item()\n",
    "        acc_t = n_correct / len(predicted) * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        total_acc += n_correct\n",
    "        total += y.shape[0]\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\t|\\tTrain Acc: {current_acc:.2f}%\")\n",
    "    logging.info(\"========================================\")\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    logging.info(f\"Train Loss: {current_loss:.2f}\")\n",
    "    logging.info(f\"Train Acc: {current_acc:.2f}\")\n",
    "    \n",
    "    # Eval trÃªn valid set\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    mb_v3.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            # Forward pass\n",
    "            output = mb_v3(X)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(output, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "            total_loss += loss_t\n",
    "\n",
    "            # Calculate Accuracies\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_correct = (predicted == y).sum().item()\n",
    "            acc_t = n_correct / len(predicted) * 100\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "            total_acc += n_correct\n",
    "\n",
    "            total += y.shape[0]\n",
    "\n",
    "            val_bar.set_postfix(loss = running_loss,\n",
    "                                acc = f\"{running_acc:.2f}%\",\n",
    "                                epoch = epoch + 1)\n",
    "            val_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(test_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "\n",
    "    val_loss.append(current_loss)\n",
    "    val_acc.append(current_acc)\n",
    "\n",
    "    print(f\"Val Loss: {current_loss:.2f}\\t|\\tVal Acc: {current_acc:.2f}%\")\n",
    "    logging.info(f\"Val Loss: {current_loss:.2f}\")\n",
    "    logging.info(f\"Val Acc: {current_acc:.2f}\")\n",
    "    \n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "\n",
    "    if epoch < 15:\n",
    "        temperature = mb_v3.display_temperature()\n",
    "        print(f\"The current temperature is: {temperature}\")\n",
    "\n",
    "    print(f\"The current learning rate is: {temp_lr}\")\n",
    "\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"Training Completed! ðŸ˜€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b32f5ac0-3e1e-4c37-b1ae-83b987bfb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"================Running Again with Kernel Num = 2===================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af535c41-45dc-4b11-bf2d-2417e4789f45",
   "metadata": {},
   "source": [
    "# Kernel Num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "276623e6-d47b-43ee-8f10-e3c3167b394e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Normal\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "The number of parameters: 1668874\n"
     ]
    }
   ],
   "source": [
    "od_bottleneck = 6\n",
    "kernel_num = 2\n",
    "temperature = 30\n",
    "num_epochs = 40\n",
    "learning_rate = 0.05\n",
    "weight_decay = 0.00004\n",
    "dropout = 0.05\n",
    "momentum = 0.9\n",
    "\n",
    "mb_v3 = mobilenetv3_small(num_classes = 10, \n",
    "                          od_bottleneck = od_bottleneck,\n",
    "                          od_outside = 0,\n",
    "                          kernel_num = kernel_num, \n",
    "                          temperature = temperature,\n",
    "                          drop_rate = dropout,\n",
    "                          use_od = True).to(args.device)\n",
    "criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "optimizer = torch.optim.SGD(mb_v3.parameters(), lr=learning_rate,\n",
    "                            weight_decay = weight_decay, momentum = momentum)\n",
    "\n",
    "print(f\"The number of parameters: {count_parameters(mb_v3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f119fb04-e8aa-4334-8a82-f5bb952bc178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0add6a811d4ed2904e457a044eb9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce9956a37f644b3a7e93cc86800ed4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80771ed652194fcab6991ae78c2c1720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\n",
      "========================================\n",
      "\u001b[1;34mEpoch 1/40\u001b[0m\n",
      "Train Loss: 1.55\t|\tTrain Acc: 41.32%\n",
      "Val Loss: 1.33\t|\tVal Acc: 52.19%\n",
      "The current temperature is: 29.033333333333335\n",
      "The current learning rate is: 0.04998766400914329\n",
      "========================================\n",
      "\u001b[1;34mEpoch 2/40\u001b[0m\n",
      "Train Loss: 1.04\t|\tTrain Acc: 62.57%\n",
      "Val Loss: 0.97\t|\tVal Acc: 65.46%\n",
      "The current temperature is: 27.164444444444445\n",
      "The current learning rate is: 0.04995066821070679\n",
      "========================================\n",
      "\u001b[1;34mEpoch 3/40\u001b[0m\n",
      "Train Loss: 0.82\t|\tTrain Acc: 71.15%\n",
      "Val Loss: 0.86\t|\tVal Acc: 70.75%\n",
      "The current temperature is: 24.548000000000002\n",
      "The current learning rate is: 0.049889049115077\n",
      "========================================\n",
      "\u001b[1;34mEpoch 4/40\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 76.83%\n",
      "Val Loss: 0.73\t|\tVal Acc: 74.80%\n",
      "The current temperature is: 21.40826666666667\n",
      "The current learning rate is: 0.04980286753286195\n",
      "========================================\n",
      "\u001b[1;34mEpoch 5/40\u001b[0m\n",
      "Train Loss: 0.55\t|\tTrain Acc: 80.97%\n",
      "Val Loss: 0.66\t|\tVal Acc: 77.83%\n",
      "The current temperature is: 18.00688888888889\n",
      "The current learning rate is: 0.04969220851487845\n",
      "========================================\n",
      "\u001b[1;34mEpoch 6/40\u001b[0m\n",
      "Train Loss: 0.47\t|\tTrain Acc: 83.54%\n",
      "Val Loss: 0.56\t|\tVal Acc: 80.69%\n",
      "The current temperature is: 14.605511111111113\n",
      "The current learning rate is: 0.049557181268217225\n",
      "========================================\n",
      "\u001b[1;34mEpoch 7/40\u001b[0m\n",
      "Train Loss: 0.42\t|\tTrain Acc: 85.52%\n",
      "Val Loss: 0.51\t|\tVal Acc: 82.52%\n",
      "The current temperature is: 11.430891851851854\n",
      "The current learning rate is: 0.049397919048468686\n",
      "========================================\n",
      "\u001b[1;34mEpoch 8/40\u001b[0m\n",
      "Train Loss: 0.37\t|\tTrain Acc: 87.27%\n",
      "Val Loss: 0.51\t|\tVal Acc: 83.00%\n",
      "The current temperature is: 8.649320691358025\n",
      "The current learning rate is: 0.04921457902821578\n",
      "========================================\n",
      "\u001b[1;34mEpoch 9/40\u001b[0m\n",
      "Train Loss: 0.34\t|\tTrain Acc: 88.19%\n",
      "Val Loss: 0.51\t|\tVal Acc: 82.70%\n",
      "The current temperature is: 6.354524483950617\n",
      "The current learning rate is: 0.04900734214192358\n",
      "========================================\n",
      "\u001b[1;34mEpoch 10/40\u001b[0m\n",
      "Train Loss: 0.30\t|\tTrain Acc: 89.45%\n",
      "Val Loss: 0.53\t|\tVal Acc: 82.88%\n",
      "The current temperature is: 4.569682989300411\n",
      "The current learning rate is: 0.048776412907378844\n",
      "========================================\n",
      "\u001b[1;34mEpoch 11/40\u001b[0m\n",
      "Train Loss: 0.28\t|\tTrain Acc: 90.14%\n",
      "Val Loss: 0.50\t|\tVal Acc: 84.31%\n",
      "The current temperature is: 3.2607992265569266\n",
      "The current learning rate is: 0.04852201922385564\n",
      "========================================\n",
      "\u001b[1;34mEpoch 12/40\u001b[0m\n",
      "Train Loss: 0.25\t|\tTrain Acc: 91.11%\n",
      "Val Loss: 0.51\t|\tVal Acc: 84.22%\n",
      "The current temperature is: 2.356479535934156\n",
      "The current learning rate is: 0.04824441214720629\n",
      "========================================\n",
      "\u001b[1;34mEpoch 13/40\u001b[0m\n",
      "Train Loss: 0.23\t|\tTrain Acc: 91.67%\n",
      "Val Loss: 0.50\t|\tVal Acc: 83.95%\n",
      "The current temperature is: 1.768671737029355\n",
      "The current learning rate is: 0.04794386564209953\n",
      "========================================\n",
      "\u001b[1;34mEpoch 14/40\u001b[0m\n",
      "Train Loss: 0.21\t|\tTrain Acc: 92.45%\n",
      "Val Loss: 0.50\t|\tVal Acc: 84.49%\n",
      "The current temperature is: 1.4099582597489893\n",
      "The current learning rate is: 0.04762067631165049\n",
      "========================================\n",
      "\u001b[1;34mEpoch 16/40\u001b[0m\n",
      "Train Loss: 0.16\t|\tTrain Acc: 94.14%\n",
      "Val Loss: 0.48\t|\tVal Acc: 85.67%\n",
      "The current learning rate is: 0.04690766700109659\n",
      "========================================\n",
      "\u001b[1;34mEpoch 17/40\u001b[0m\n",
      "Train Loss: 0.15\t|\tTrain Acc: 94.55%\n",
      "Val Loss: 0.50\t|\tVal Acc: 85.16%\n",
      "The current learning rate is: 0.046518550675098594\n",
      "========================================\n",
      "\u001b[1;34mEpoch 18/40\u001b[0m\n",
      "Train Loss: 0.13\t|\tTrain Acc: 95.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 19/40\u001b[0m\n",
      "Train Loss: 0.12\t|\tTrain Acc: 95.78%\n",
      "Val Loss: 0.54\t|\tVal Acc: 85.56%\n",
      "The current learning rate is: 0.04567701435686405\n",
      "========================================\n",
      "\u001b[1;34mEpoch 20/40\u001b[0m\n",
      "Train Loss: 0.11\t|\tTrain Acc: 96.27%\n",
      "Val Loss: 0.56\t|\tVal Acc: 85.38%\n",
      "The current learning rate is: 0.04522542485937369\n",
      "========================================\n",
      "\u001b[1;34mEpoch 21/40\u001b[0m\n",
      "Train Loss: 0.10\t|\tTrain Acc: 96.63%\n",
      "Val Loss: 0.60\t|\tVal Acc: 85.07%\n",
      "The current learning rate is: 0.04475387530939226\n",
      "========================================\n",
      "\u001b[1;34mEpoch 22/40\u001b[0m\n",
      "Train Loss: 0.09\t|\tTrain Acc: 96.84%\n",
      "Val Loss: 0.53\t|\tVal Acc: 86.24%\n",
      "The current learning rate is: 0.044262831069394735\n",
      "========================================\n",
      "\u001b[1;34mEpoch 23/40\u001b[0m\n",
      "Train Loss: 0.09\t|\tTrain Acc: 96.96%\n",
      "Val Loss: 0.54\t|\tVal Acc: 86.62%\n",
      "The current learning rate is: 0.043752776740761494\n",
      "========================================\n",
      "\u001b[1;34mEpoch 24/40\u001b[0m\n",
      "Train Loss: 0.07\t|\tTrain Acc: 97.48%\n",
      "Val Loss: 0.54\t|\tVal Acc: 86.66%\n",
      "The current learning rate is: 0.04322421568553529\n",
      "========================================\n",
      "\u001b[1;34mEpoch 25/40\u001b[0m\n",
      "Train Loss: 0.07\t|\tTrain Acc: 97.70%\n",
      "Val Loss: 0.59\t|\tVal Acc: 86.35%\n",
      "The current learning rate is: 0.04267766952966369\n",
      "========================================\n",
      "\u001b[1;34mEpoch 26/40\u001b[0m\n",
      "Train Loss: 0.06\t|\tTrain Acc: 97.75%\n",
      "Val Loss: 0.63\t|\tVal Acc: 85.83%\n",
      "The current learning rate is: 0.04211367764821722\n",
      "========================================\n",
      "\u001b[1;34mEpoch 27/40\u001b[0m\n",
      "Train Loss: 0.06\t|\tTrain Acc: 98.02%\n",
      "Val Loss: 0.61\t|\tVal Acc: 86.45%\n",
      "The current learning rate is: 0.0415327966330913\n",
      "========================================\n",
      "\u001b[1;34mEpoch 28/40\u001b[0m\n",
      "Train Loss: 0.05\t|\tTrain Acc: 98.18%\n",
      "Val Loss: 0.56\t|\tVal Acc: 87.51%\n",
      "The current learning rate is: 0.040935599743717244\n",
      "========================================\n",
      "\u001b[1;34mEpoch 29/40\u001b[0m\n",
      "Train Loss: 0.05\t|\tTrain Acc: 98.38%\n",
      "Val Loss: 0.58\t|\tVal Acc: 87.02%\n",
      "The current learning rate is: 0.040322676341324415\n",
      "========================================\n",
      "\u001b[1;34mEpoch 30/40\u001b[0m\n",
      "Train Loss: 0.04\t|\tTrain Acc: 98.48%\n",
      "Val Loss: 0.60\t|\tVal Acc: 87.07%\n",
      "The current learning rate is: 0.03969463130731183\n",
      "========================================\n",
      "\u001b[1;34mEpoch 31/40\u001b[0m\n",
      "Train Loss: 0.03\t|\tTrain Acc: 98.99%\n",
      "Val Loss: 0.59\t|\tVal Acc: 87.46%\n",
      "The current learning rate is: 0.03905208444630327\n",
      "========================================\n",
      "\u001b[1;34mEpoch 32/40\u001b[0m\n",
      "Train Loss: 0.03\t|\tTrain Acc: 98.84%\n",
      "Val Loss: 0.58\t|\tVal Acc: 87.45%\n",
      "The current learning rate is: 0.038395669874474916\n",
      "========================================\n",
      "\u001b[1;34mEpoch 33/40\u001b[0m\n",
      "Train Loss: 0.02\t|\tTrain Acc: 99.16%\n",
      "Val Loss: 0.63\t|\tVal Acc: 87.45%\n",
      "The current learning rate is: 0.03772603539375929\n",
      "========================================\n",
      "\u001b[1;34mEpoch 34/40\u001b[0m\n",
      "Train Loss: 0.02\t|\tTrain Acc: 99.31%\n",
      "Val Loss: 0.64\t|\tVal Acc: 87.51%\n",
      "The current learning rate is: 0.037043841852542884\n",
      "========================================\n",
      "\u001b[1;34mEpoch 35/40\u001b[0m\n",
      "Train Loss: 0.03\t|\tTrain Acc: 99.13%\n",
      "Val Loss: 0.60\t|\tVal Acc: 88.02%\n",
      "The current learning rate is: 0.03634976249348867\n",
      "========================================\n",
      "\u001b[1;34mEpoch 36/40\u001b[0m\n",
      "Train Loss: 0.03\t|\tTrain Acc: 98.96%\n",
      "Val Loss: 0.62\t|\tVal Acc: 87.41%\n",
      "The current learning rate is: 0.03564448228912682\n",
      "========================================\n",
      "\u001b[1;34mEpoch 37/40\u001b[0m\n",
      "Train Loss: 0.03\t|\tTrain Acc: 98.99%\n",
      "Val Loss: 0.65\t|\tVal Acc: 87.10%\n",
      "The current learning rate is: 0.03492869726586952\n",
      "========================================\n",
      "\u001b[1;34mEpoch 38/40\u001b[0m\n",
      "Train Loss: 0.02\t|\tTrain Acc: 99.40%\n",
      "Val Loss: 0.60\t|\tVal Acc: 88.15%\n",
      "The current learning rate is: 0.03420311381711695\n",
      "========================================\n",
      "\u001b[1;34mEpoch 39/40\u001b[0m\n",
      "Train Loss: 0.01\t|\tTrain Acc: 99.68%\n",
      "Val Loss: 0.62\t|\tVal Acc: 88.23%\n",
      "The current learning rate is: 0.033468448006132294\n",
      "========================================\n",
      "\u001b[1;34mEpoch 40/40\u001b[0m\n",
      "Train Loss: 0.01\t|\tTrain Acc: 99.79%\n",
      "Val Loss: 0.62\t|\tVal Acc: 88.75%\n",
      "The current learning rate is: 0.032725424859373686\n",
      "========================================\n",
      "Training Completed! ðŸ˜€\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "               position = 1, leave = True)\n",
    "print(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    mb_v3.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "        if epoch < 30:\n",
    "            temp = get_temperature(i + 1, epoch, len(train_loader),\n",
    "                                   temp_epoch = 30, temp_init = temperature)\n",
    "            mb_v3.net_update_temperature(temp)\n",
    "            # print(f\"The temperature is: {mb_v3.display_temperature()}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        X, y = X.to(args.device), y.to(args.device)\n",
    "\n",
    "        # X, y_origin, y_sampled, lam = mixup_data(X, y, args.device,\n",
    "        #                                          alpha = 0.4)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = mb_v3(X)\n",
    "        loss = criterion(output, y)\n",
    "        # loss = mixup_criterion(criterion, output, y_origin, y_sampled, lam)\n",
    "\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        temp_lr = adjust_learning_rate(optimizer, epoch, 100,\n",
    "                                       i + 1, len(train_loader),\n",
    "                                       initial_lr = learning_rate)\n",
    "\n",
    "        # Calculating the accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        # n_correct = (lam * predicted.eq(y_origin.data).cpu().sum().float()\n",
    "        #             + (1 - lam) * predicted.eq(y_sampled.data).cpu().sum().float())\n",
    "        n_correct = (predicted == y).sum().item()\n",
    "        acc_t = n_correct / len(predicted) * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        total_acc += n_correct\n",
    "        total += y.shape[0]\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\t|\\tTrain Acc: {current_acc:.2f}%\")\n",
    "    logging.info(\"========================================\")\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    logging.info(f\"Train Loss: {current_loss:.2f}\")\n",
    "    logging.info(f\"Train Acc: {current_acc:.2f}\")\n",
    "    \n",
    "    # Eval trÃªn valid set\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    mb_v3.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            # Forward pass\n",
    "            output = mb_v3(X)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(output, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "            total_loss += loss_t\n",
    "\n",
    "            # Calculate Accuracies\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_correct = (predicted == y).sum().item()\n",
    "            acc_t = n_correct / len(predicted) * 100\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "            total_acc += n_correct\n",
    "\n",
    "            total += y.shape[0]\n",
    "\n",
    "            val_bar.set_postfix(loss = running_loss,\n",
    "                                acc = f\"{running_acc:.2f}%\",\n",
    "                                epoch = epoch + 1)\n",
    "            val_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(test_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "\n",
    "    val_loss.append(current_loss)\n",
    "    val_acc.append(current_acc)\n",
    "\n",
    "    print(f\"Val Loss: {current_loss:.2f}\\t|\\tVal Acc: {current_acc:.2f}%\")\n",
    "    logging.info(f\"Val Loss: {current_loss:.2f}\")\n",
    "    logging.info(f\"Val Acc: {current_acc:.2f}\")\n",
    "    \n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "\n",
    "    if epoch < 15:\n",
    "        temperature = mb_v3.display_temperature()\n",
    "        print(f\"The current temperature is: {temperature}\")\n",
    "\n",
    "    print(f\"The current learning rate is: {temp_lr}\")\n",
    "\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"Training Completed! ðŸ˜€\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a5083-2760-44e1-8487-4287f4a048a2",
   "metadata": {},
   "source": [
    "# Testing with MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc712140-56bf-459e-80cd-466b6c87386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"======================= Testing with MixUp 50 epochs OD_max Kernel = 2 ======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98afe454-40fc-40cf-82fa-d5b9a6d61be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Normal\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using OmniDimensional\n",
      "Using Normal\n",
      "Using Normal\n",
      "Using Normal\n",
      "The number of parameters: 2260295\n"
     ]
    }
   ],
   "source": [
    "od_bottleneck = 11\n",
    "kernel_num = 2\n",
    "temperature = 30\n",
    "num_epochs = 50\n",
    "learning_rate = 0.05\n",
    "weight_decay = 0.00004\n",
    "dropout = 0.05\n",
    "momentum = 0.9\n",
    "\n",
    "mb_v3 = mobilenetv3_small(num_classes = 10, \n",
    "                          od_bottleneck = od_bottleneck,\n",
    "                          od_outside = 0,\n",
    "                          kernel_num = kernel_num, \n",
    "                          temperature = temperature,\n",
    "                          drop_rate = dropout,\n",
    "                          use_od = True).to(args.device)\n",
    "criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "optimizer = torch.optim.SGD(mb_v3.parameters(), lr=learning_rate,\n",
    "                            weight_decay = weight_decay, momentum = momentum)\n",
    "\n",
    "print(f\"The number of parameters: {count_parameters(mb_v3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04486394-3c59-4ae3-a73a-2207de24789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def mixup_data(x, y, device, alpha = 1.0):\n",
    "  if alpha > 0:\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "  else:\n",
    "    lam = 1\n",
    "\n",
    "  batch_size = x.shape[0]\n",
    "  index_sample = torch.randperm(batch_size).to(device)\n",
    "\n",
    "  mixed_x = lam * x + (1 - lam) * x[index_sample, :]\n",
    "  y, y_sampled =  y, y[index_sample]\n",
    "\n",
    "  return mixed_x, y, y_sampled, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "  return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ceee9f6-07ae-4d61-940b-8f68954438d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, total_epochs,\n",
    "                         iteration, iter_per_epoch, initial_lr = 0.05):\n",
    "    current_iter = iteration + epoch * iter_per_epoch\n",
    "    max_iter = total_epochs * iter_per_epoch\n",
    "\n",
    "    lr = initial_lr * (1 + np.cos(np.pi * current_iter / max_iter)) / 2\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def get_temperature(iteration, epoch, iter_per_epoch,\n",
    "                        temp_epoch = 10, temp_init = 30.0):\n",
    "    total_temp_iter = iter_per_epoch * temp_epoch\n",
    "    current_iter = iteration + epoch * iter_per_epoch\n",
    "    # print(current_iter)\n",
    "    temperature = 1.0 + max(0, (temp_init - 1.0) * \\\n",
    "                            ((total_temp_iter - current_iter) / \\\n",
    "                            total_temp_iter))\n",
    "    return temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a673c47-92b5-4fd6-b0b3-0ad6c09af00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b81b1d12f34ec1a41a709b7abce000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f27677e40446daba031835c7c931c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507c5ddfbaa243da89f0fbef315d9cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\n",
      "========================================\n",
      "\u001b[1;34mEpoch 1/50\u001b[0m\n",
      "Train Loss: 1.76\t|\tTrain Acc: 37.52%\n",
      "Val Loss: 1.53\t|\tVal Acc: 46.05%\n",
      "The current temperature is: 29.033333333333335\n",
      "The current learning rate is: 0.04998766400914329\n",
      "========================================\n",
      "\u001b[1;34mEpoch 2/50\u001b[0m\n",
      "Train Loss: 1.44\t|\tTrain Acc: 53.02%\n",
      "Val Loss: 1.05\t|\tVal Acc: 64.17%\n",
      "The current temperature is: 27.164444444444445\n",
      "The current learning rate is: 0.04995066821070679\n",
      "========================================\n",
      "\u001b[1;34mEpoch 3/50\u001b[0m\n",
      "Train Loss: 1.28\t|\tTrain Acc: 60.07%\n",
      "Val Loss: 0.87\t|\tVal Acc: 71.73%\n",
      "The current temperature is: 24.548000000000002\n",
      "The current learning rate is: 0.049889049115077\n",
      "========================================\n",
      "\u001b[1;34mEpoch 4/50\u001b[0m\n",
      "Train Loss: 1.18\t|\tTrain Acc: 64.18%\n",
      "Val Loss: 0.71\t|\tVal Acc: 77.40%\n",
      "The current temperature is: 21.40826666666667\n",
      "The current learning rate is: 0.04980286753286195\n",
      "========================================\n",
      "\u001b[1;34mEpoch 5/50\u001b[0m\n",
      "Train Loss: 1.13\t|\tTrain Acc: 66.27%\n",
      "Val Loss: 0.66\t|\tVal Acc: 79.84%\n",
      "The current temperature is: 18.00688888888889\n",
      "The current learning rate is: 0.04969220851487845\n",
      "========================================\n",
      "\u001b[1;34mEpoch 6/50\u001b[0m\n",
      "Train Loss: 1.07\t|\tTrain Acc: 68.83%\n",
      "Val Loss: 0.68\t|\tVal Acc: 78.49%\n",
      "The current temperature is: 14.605511111111113\n",
      "The current learning rate is: 0.049557181268217225\n",
      "========================================\n",
      "\u001b[1;34mEpoch 7/50\u001b[0m\n",
      "Train Loss: 1.02\t|\tTrain Acc: 70.82%\n",
      "Val Loss: 0.62\t|\tVal Acc: 81.82%\n",
      "The current temperature is: 11.430891851851854\n",
      "The current learning rate is: 0.049397919048468686\n",
      "========================================\n",
      "\u001b[1;34mEpoch 8/50\u001b[0m\n",
      "Train Loss: 0.98\t|\tTrain Acc: 72.19%\n",
      "Val Loss: 0.54\t|\tVal Acc: 83.59%\n",
      "The current temperature is: 8.649320691358025\n",
      "The current learning rate is: 0.04921457902821578\n",
      "========================================\n",
      "\u001b[1;34mEpoch 9/50\u001b[0m\n",
      "Train Loss: 0.95\t|\tTrain Acc: 73.48%\n",
      "Val Loss: 0.50\t|\tVal Acc: 84.20%\n",
      "The current temperature is: 6.354524483950617\n",
      "The current learning rate is: 0.04900734214192358\n",
      "========================================\n",
      "\u001b[1;34mEpoch 10/50\u001b[0m\n",
      "Train Loss: 0.96\t|\tTrain Acc: 73.35%\n",
      "Val Loss: 0.60\t|\tVal Acc: 83.43%\n",
      "The current temperature is: 4.569682989300411\n",
      "The current learning rate is: 0.048776412907378844\n",
      "========================================\n",
      "\u001b[1;34mEpoch 11/50\u001b[0m\n",
      "Train Loss: 0.91\t|\tTrain Acc: 74.52%\n",
      "Val Loss: 0.51\t|\tVal Acc: 84.60%\n",
      "The current temperature is: 3.2607992265569266\n",
      "The current learning rate is: 0.04852201922385564\n",
      "========================================\n",
      "\u001b[1;34mEpoch 12/50\u001b[0m\n",
      "Train Loss: 0.89\t|\tTrain Acc: 75.62%\n",
      "Val Loss: 0.52\t|\tVal Acc: 84.88%\n",
      "The current temperature is: 2.356479535934156\n",
      "The current learning rate is: 0.04824441214720629\n",
      "========================================\n",
      "\u001b[1;34mEpoch 13/50\u001b[0m\n",
      "Train Loss: 0.90\t|\tTrain Acc: 74.88%\n",
      "Val Loss: 0.48\t|\tVal Acc: 85.07%\n",
      "The current temperature is: 1.768671737029355\n",
      "The current learning rate is: 0.04794386564209953\n",
      "========================================\n",
      "\u001b[1;34mEpoch 14/50\u001b[0m\n",
      "Train Loss: 0.87\t|\tTrain Acc: 76.15%\n",
      "Val Loss: 0.49\t|\tVal Acc: 86.22%\n",
      "The current temperature is: 1.4099582597489893\n",
      "The current learning rate is: 0.04762067631165049\n",
      "========================================\n",
      "\u001b[1;34mEpoch 15/50\u001b[0m\n",
      "Train Loss: 0.88\t|\tTrain Acc: 75.33%\n",
      "Val Loss: 0.46\t|\tVal Acc: 86.24%\n",
      "The current temperature is: 1.2049791298744945\n",
      "The current learning rate is: 0.047275163104709195\n",
      "========================================\n",
      "\u001b[1;34mEpoch 16/50\u001b[0m\n",
      "Train Loss: 0.84\t|\tTrain Acc: 77.47%\n",
      "Val Loss: 0.47\t|\tVal Acc: 86.86%\n",
      "The current temperature is: 1.095656927274764\n",
      "The current learning rate is: 0.04690766700109659\n",
      "========================================\n",
      "\u001b[1;34mEpoch 17/50\u001b[0m\n",
      "Train Loss: 0.82\t|\tTrain Acc: 77.98%\n",
      "Val Loss: 0.41\t|\tVal Acc: 87.23%\n",
      "The current temperature is: 1.0414513351523977\n",
      "The current learning rate is: 0.046518550675098594\n",
      "========================================\n",
      "\u001b[1;34mEpoch 18/50\u001b[0m\n",
      "Train Loss: 0.86\t|\tTrain Acc: 77.04%\n",
      "Val Loss: 0.46\t|\tVal Acc: 88.11%\n",
      "The current temperature is: 1.016580534060959\n",
      "The current learning rate is: 0.04610819813755038\n",
      "========================================\n",
      "\u001b[1;34mEpoch 19/50\u001b[0m\n",
      "Train Loss: 0.81\t|\tTrain Acc: 78.45%\n",
      "Val Loss: 0.41\t|\tVal Acc: 87.52%\n",
      "The current temperature is: 1.006079529155685\n",
      "The current learning rate is: 0.04567701435686405\n",
      "========================================\n",
      "\u001b[1;34mEpoch 20/50\u001b[0m\n",
      "Train Loss: 0.77\t|\tTrain Acc: 79.51%\n",
      "Val Loss: 0.43\t|\tVal Acc: 87.71%\n",
      "The current temperature is: 1.0020265097185617\n",
      "The current learning rate is: 0.04522542485937369\n",
      "========================================\n",
      "\u001b[1;34mEpoch 21/50\u001b[0m\n",
      "Train Loss: 0.77\t|\tTrain Acc: 79.63%\n",
      "Val Loss: 0.46\t|\tVal Acc: 87.32%\n",
      "The current temperature is: 1.0006079529155685\n",
      "The current learning rate is: 0.04475387530939226\n",
      "========================================\n",
      "\u001b[1;34mEpoch 22/50\u001b[0m\n",
      "Train Loss: 0.77\t|\tTrain Acc: 79.60%\n",
      "Val Loss: 0.42\t|\tVal Acc: 88.25%\n",
      "The current temperature is: 1.000162120777485\n",
      "The current learning rate is: 0.044262831069394735\n",
      "========================================\n",
      "\u001b[1;34mEpoch 23/50\u001b[0m\n",
      "Train Loss: 0.81\t|\tTrain Acc: 78.11%\n",
      "Val Loss: 0.44\t|\tVal Acc: 87.96%\n",
      "The current temperature is: 1.000037828181413\n",
      "The current learning rate is: 0.043752776740761494\n",
      "========================================\n",
      "\u001b[1;34mEpoch 24/50\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 81.08%\n",
      "Val Loss: 0.43\t|\tVal Acc: 88.10%\n",
      "The current temperature is: 1.0000075656362826\n",
      "The current learning rate is: 0.04322421568553529\n",
      "========================================\n",
      "\u001b[1;34mEpoch 25/50\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 80.49%\n",
      "Val Loss: 0.41\t|\tVal Acc: 89.09%\n",
      "The current temperature is: 1.0000012609393805\n",
      "The current learning rate is: 0.04267766952966369\n",
      "========================================\n",
      "\u001b[1;34mEpoch 26/50\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 81.17%\n",
      "Val Loss: 0.44\t|\tVal Acc: 88.37%\n",
      "The current temperature is: 1.0000001681252508\n",
      "The current learning rate is: 0.04211367764821722\n",
      "========================================\n",
      "\u001b[1;34mEpoch 27/50\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 80.91%\n",
      "Val Loss: 0.41\t|\tVal Acc: 88.42%\n",
      "The current temperature is: 1.0000000168125251\n",
      "The current learning rate is: 0.0415327966330913\n",
      "========================================\n",
      "\u001b[1;34mEpoch 28/50\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 81.46%\n",
      "Val Loss: 0.40\t|\tVal Acc: 88.63%\n",
      "The current temperature is: 1.000000001120835\n",
      "The current learning rate is: 0.040935599743717244\n",
      "========================================\n",
      "\u001b[1;34mEpoch 29/50\u001b[0m\n",
      "Train Loss: 0.69\t|\tTrain Acc: 82.12%\n",
      "Val Loss: 0.46\t|\tVal Acc: 87.91%\n",
      "The current temperature is: 1.0000000000373612\n",
      "The current learning rate is: 0.040322676341324415\n",
      "========================================\n",
      "\u001b[1;34mEpoch 31/50\u001b[0m\n",
      "Train Loss: 0.73\t|\tTrain Acc: 80.67%\n",
      "Val Loss: 0.38\t|\tVal Acc: 89.14%\n",
      "The current learning rate is: 0.03905208444630327\n",
      "========================================\n",
      "\u001b[1;34mEpoch 32/50\u001b[0m\n",
      "Train Loss: 0.70\t|\tTrain Acc: 82.13%\n",
      "Val Loss: 0.41\t|\tVal Acc: 89.40%\n",
      "The current learning rate is: 0.038395669874474916\n",
      "========================================\n",
      "\u001b[1;34mEpoch 33/50\u001b[0m\n",
      "Train Loss: 0.72\t|\tTrain Acc: 81.11%\n",
      "Val Loss: 0.40\t|\tVal Acc: 89.38%\n",
      "The current learning rate is: 0.03772603539375929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 48/50\u001b[0m\n",
      "Train Loss: 0.69\t|\tTrain Acc: 81.15%\n",
      "Val Loss: 0.40\t|\tVal Acc: 89.90%\n",
      "The current learning rate is: 0.026569762988232833\n",
      "========================================\n",
      "\u001b[1;34mEpoch 49/50\u001b[0m\n",
      "Train Loss: 0.66\t|\tTrain Acc: 82.44%\n",
      "Val Loss: 0.38\t|\tVal Acc: 89.94%\n",
      "The current learning rate is: 0.02578526897695321\n",
      "========================================\n",
      "\u001b[1;34mEpoch 50/50\u001b[0m\n",
      "Train Loss: 0.67\t|\tTrain Acc: 82.45%\n",
      "Val Loss: 0.39\t|\tVal Acc: 90.59%\n",
      "The current learning rate is: 0.025\n",
      "========================================\n",
      "Training Completed! ðŸ˜€\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "               position = 1, leave = True)\n",
    "print(\"ðŸš€ Training MobileNetV3 - Omni Dimensional Dynamic Convolution ðŸš€\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    mb_v3.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "        if epoch < 30:\n",
    "            temp = get_temperature(i + 1, epoch, len(train_loader),\n",
    "                                   temp_epoch = 30, temp_init = temperature)\n",
    "            mb_v3.net_update_temperature(temp)\n",
    "            # print(f\"The temperature is: {mb_v3.display_temperature()}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        X, y = X.to(args.device), y.to(args.device)\n",
    "\n",
    "        X, y_origin, y_sampled, lam = mixup_data(X, y, args.device,\n",
    "                                                 alpha = 0.2)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = mb_v3(X)\n",
    "        # loss = criterion(output, y)\n",
    "        loss = mixup_criterion(criterion, output, y_origin, y_sampled, lam)\n",
    "\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        temp_lr = adjust_learning_rate(optimizer, epoch, 100,\n",
    "                                       i + 1, len(train_loader),\n",
    "                                       initial_lr = learning_rate)\n",
    "\n",
    "        # Calculating the accuracy\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        n_correct = (lam * predicted.eq(y_origin.data).cpu().sum().float()\n",
    "                    + (1 - lam) * predicted.eq(y_sampled.data).cpu().sum().float())\n",
    "        # n_correct = (predicted == y).sum().item()\n",
    "        acc_t = n_correct / len(predicted) * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "\n",
    "        total_acc += n_correct\n",
    "        total += y.shape[0]\n",
    "\n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\t|\\tTrain Acc: {current_acc:.2f}%\")\n",
    "    logging.info(\"========================================\")\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    logging.info(f\"Train Loss: {current_loss:.2f}\")\n",
    "    logging.info(f\"Train Acc: {current_acc:.2f}%\")\n",
    "    \n",
    "    # Eval trÃªn valid set\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    total = 0\n",
    "    mb_v3.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_loader):\n",
    "\n",
    "            X, y = X.to(args.device), y.to(args.device)\n",
    "            # Forward pass\n",
    "            output = mb_v3(X)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(output, y)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (i + 1)\n",
    "            total_loss += loss_t\n",
    "\n",
    "            # Calculate Accuracies\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            n_correct = (predicted == y).sum().item()\n",
    "            acc_t = n_correct / len(predicted) * 100\n",
    "            running_acc += (acc_t - running_acc) / (i + 1)\n",
    "            total_acc += n_correct\n",
    "\n",
    "            total += y.shape[0]\n",
    "\n",
    "            val_bar.set_postfix(loss = running_loss,\n",
    "                                acc = f\"{running_acc:.2f}%\",\n",
    "                                epoch = epoch + 1)\n",
    "            val_bar.update()\n",
    "\n",
    "    current_loss = total_loss / len(test_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "\n",
    "    val_loss.append(current_loss)\n",
    "    val_acc.append(current_acc)\n",
    "\n",
    "    print(f\"Val Loss: {current_loss:.2f}\\t|\\tVal Acc: {current_acc:.2f}%\")\n",
    "    logging.info(f\"Val Loss: {current_loss:.2f}\")\n",
    "    logging.info(f\"Val Acc: {current_acc:.2f}%\")\n",
    "    \n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.update()\n",
    "\n",
    "    if epoch < 30:\n",
    "        temperature = mb_v3.display_temperature()\n",
    "        print(f\"The current temperature is: {temperature}\")\n",
    "\n",
    "    print(f\"The current learning rate is: {temp_lr}\")\n",
    "\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"Training Completed! ðŸ˜€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eacc911e-0b0f-4721-b798-4584a335612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fab9bb-1a46-40f0-a2b9-3e238a55626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
